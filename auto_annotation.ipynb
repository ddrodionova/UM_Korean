{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from os import listdir\n",
    "import fcntl\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "raw_dr = './raw_tables/Korean'\n",
    "new_raw = './nnew_auto_raw_tables/'\n",
    "dr = './annotated_tables/Korean'\n",
    "newdr = './nnew_pred_annotated_tables'\n",
    "cor_ann = './nnew_auto_annotated_tables/'\n",
    "if not os.path.exists(new_raw):\n",
    "    os.makedirs(new_raw)\n",
    "if not os.path.exists(newdr):\n",
    "    os.makedirs(newdr)\n",
    "if not os.path.exists(cor_ann):\n",
    "    os.makedirs(cor_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copies(pos, dr, new_raw, smp, newdr):\n",
    "    \n",
    "    for f in listdir(new_raw):\n",
    "        if f in ['ADJ_000001_(21,8)_example.csv', 'ADJ_000001_(30,8)_example.csv']:\n",
    "            shutil.copy(join(dr, f), join(newdr, f))\n",
    "        if pos in f and 'csv' in f:\n",
    "            if 'lock' not in f:\n",
    "                shutil.copy(join(dr, smp), join(newdr, f))\n",
    "            else:\n",
    "                fcntl.flock(join(dr, f), fcntl.LOCK_UN)\n",
    "                shutil.copy(join(dr, smp), join(newdr, f[7:-1]))\n",
    "\n",
    "                \n",
    "def clean_raw(raw_dr, new_raw):\n",
    "    for f in listdir(raw_dr):\n",
    "        df = pd.read_csv(join(raw_dr, f), sep=None, engine='python')\n",
    "        \n",
    "        f, pos, ck = check_pos(df, f, new_raw)\n",
    "        if not ck:\n",
    "            print(f, '- too many tables')\n",
    "            continue\n",
    "        \n",
    "        for i, row in enumerate(df.iterrows()):\n",
    "            if 'Formal non-polite(해라체)' in row[1].unique():\n",
    "                if pos == 'ADJ':\n",
    "                    st = i\n",
    "                    en = i + 34\n",
    "                if pos == 'V':\n",
    "                    st = i\n",
    "                    en = i + 40\n",
    "                break\n",
    "                \n",
    "        df = pd.DataFrame([i[1].values[0:6] for i in list(df.iterrows())[st:en]])\n",
    "        \n",
    "        for i in df.columns:\n",
    "            df[i] = df[i].str.replace(u'(([\\u3131-\\u3163\\uac00-\\ud7a3]+, )?[\\u3131-\\u3163\\uac00-\\ud7a3]+)(-?[a-z, ]+)', u'\\g<1>', regex=True)\n",
    "        \n",
    "        df.to_csv(join(new_raw, f), header=False, index=False)\n",
    "        \n",
    "\n",
    "def check_pos(df, fn, new_raw):\n",
    "    print(fn)\n",
    "    if df[df.columns[0]].value_counts()['Indicative'] > 4:\n",
    "        return fn, None, False\n",
    "    if 'Imperative' in df[df.columns[0]].values or 'Hortative' in df[df.columns[0]].values or 'Motive' in df[df.columns[0]].values:\n",
    "        pos = 'V'\n",
    "    else:\n",
    "        pos = 'ADJ'\n",
    "    nfn = fn.split('_')\n",
    "    nfn[0] = pos\n",
    "    nfn = '_'.join(nfn)\n",
    "    if nfn in listdir(new_raw):\n",
    "        nfn = nfn[:-4] + '_' + str(len(listdir(new_raw))) + '.csv'\n",
    "    return nfn, pos, True\n",
    "\n",
    "\n",
    "def correct_anno(newdr, new_raw, cor_ann):\n",
    "    \n",
    "    for f in listdir(newdr):\n",
    "        if 'tsv' not in f and 'csv' not in f or f  in ['ADJ_000001_(21,8)_example.csv', 'ADJ_000001_(30,8)_example.csv']:\n",
    "            continue\n",
    "        df1 = pd.read_csv(join(newdr, f), sep=None, engine='python')\n",
    "        df2 = pd.read_csv(join(new_raw, f), sep=None, engine='python')\n",
    "            \n",
    "        for i in df2.columns:\n",
    "            nv = []\n",
    "            if len(df2[i]) > len(df1[i]):\n",
    "                df2 = df2.drop(range(len(df1[i]), len(df2[i])), axis=0)\n",
    "            for j, rw in enumerate(df2[i]):\n",
    "                anno = df1[i][j]\n",
    "                if str(anno) == 'nan':\n",
    "                    nv.append(anno)\n",
    "                    continue\n",
    "                an_0 = str(anno).split(', ')[0]\n",
    "                nv.append(', '.join([an_0] * len(str(rw).split(', '))))\n",
    "            if len(df1[i]) > len(df2[i]):\n",
    "                df1 = df1.drop(range(len(df2[i]), len(df1[i])), axis=0)\n",
    "            df1[i] = nv\n",
    "        df1.to_csv(join(cor_ann, f), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_000001_(137,8)_example.csv\n",
      "V_000001_(137,8)_example.csv - too many tables\n",
      "V_000002_(31,8)_example.csv\n",
      "ADJ_000005_(49,8)_example.csv\n",
      "V_000001_(76,14)_example.csv\n",
      "V_000001_(63,8)_example.csv\n",
      "ADJ_000006_(43,6)_example.csv\n",
      "ADJ_000001_(137,8)_example.csv\n",
      "ADJ_000001_(137,8)_example.csv - too many tables\n",
      "V_000003_(47,10)_example.csv\n",
      "V_000001_(101,8)_example.csv\n",
      "V_000001_(101,8)_example.csv - too many tables\n",
      "V_000001_(49,8)_example.csv\n",
      "V_000001_(62,8)_example.csv\n",
      "ADJ_000002_(73,8)_example.csv\n",
      "V_000014_(47,8)_example.csv\n",
      "V_000002_(80,8)_example.csv\n",
      "V_000002_(80,8)_example.csv - too many tables\n",
      "V_000001_(71,8)_example.csv\n",
      "V_000002_(99,8)_example.csv\n",
      "V_000002_(99,8)_example.csv - too many tables\n",
      "ADJ_000001_(55,16)_example.csv\n",
      "V_000001_(56,10)_example.csv\n",
      "V_000001_(38,8)_example.csv\n",
      "ADJ_000099_(38,8)_example.csv\n",
      "V_000233_(45,8)_example.csv\n",
      "V_000037_(56,8)_example.csv\n",
      "ADJ_000008_(94,8)_example.csv\n",
      "ADJ_000008_(94,8)_example.csv - too many tables\n",
      "V_000002_(53,8)_example.csv\n",
      "V_000001_(75,14)_example.csv\n",
      "N_000001_(47,8)_example.csv\n",
      "ADJ_000340_(48,8)_example.csv\n",
      "ADJ_000001_(93,8)_example.csv\n",
      "ADJ_000001_(93,8)_example.csv - too many tables\n",
      "V_001153_(55,8)_example.csv\n",
      "V_000001_(108,8)_example.csv\n",
      "V_000001_(108,8)_example.csv - too many tables\n",
      "V_000008_(94,8)_example.csv\n",
      "V_000008_(94,8)_example.csv - too many tables\n",
      "ADJ_000002_(52,8)_example.csv\n",
      "ADJ_000001_(99,8)_example.csv\n",
      "ADJ_000001_(99,8)_example.csv - too many tables\n",
      "ADJ_000002_(39,8)_example.csv\n",
      "ADJ_000003_(69,14)_example.csv\n",
      "ADJ_000001_(21,8)_example.csv\n",
      "ADJ_000002_(45,8)_example.csv\n",
      "ADJ_000001_(53,12)_example.csv\n",
      "ADJ_000001_(40,8)_example.csv\n",
      "N_000001_(41,6)_example.csv\n",
      "V_000008_(34,8)_example.csv\n",
      "N_000002_(45,8)_example.csv\n",
      "V_000143_(50,6)_example.csv\n",
      "V_000002_(95,8)_example.csv\n",
      "V_000002_(95,8)_example.csv - too many tables\n",
      "ADJ_000001_(46,16)_example.csv\n",
      "V_000001_(93,8)_example.csv\n",
      "V_000001_(93,8)_example.csv - too many tables\n",
      "ADJ_000221_(47,8)_example.csv\n",
      "V_000003_(59,8)_example.csv\n",
      "ADJ_000006_(92,8)_example.csv\n",
      "ADJ_000006_(92,8)_example.csv - too many tables\n",
      "V_000001_(57,8)_example.csv\n",
      "V_000008_(46,8)_example.csv\n",
      "ADJ_000001_(48,9)_example.csv\n",
      "V_000003_(48,8)_example.csv\n",
      "ADJ_000001_(30,8)_example.csv\n",
      "V_000004_(60,8)_example.csv\n",
      "V_000001_(58,8)_example.csv\n",
      "V_000357_(54,8)_example.csv\n",
      "V_000006_(92,8)_example.csv\n",
      "V_000006_(92,8)_example.csv - too many tables\n",
      "ADJ_000001_(71,8)_example.csv\n",
      "V_000003_(61,8)_example.csv\n",
      "ADJ_000004_(55,8)_example.csv\n",
      "ADJ_000001_(68,14)_example.csv\n",
      "V_000001_(50,6)_example.csv\n",
      "V_000004_(35,8)_example.csv\n",
      "ADJ_000002_(95,8)_example.csv\n",
      "ADJ_000002_(95,8)_example.csv - too many tables\n"
     ]
    }
   ],
   "source": [
    "clean_raw(raw_dr, new_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp = 'ADJ_000001_(40,8)_example.csv'\n",
    "pos = 'ADJ'\n",
    "make_copies(pos, dr, new_raw, smp, newdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp = 'verb_paradigm.csv'\n",
    "pos = 'V'\n",
    "make_copies(pos, dr, new_raw, smp, newdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_anno(newdr, new_raw, cor_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['ADJ_000001_(21,8)_example.csv', 'ADJ_000001_(30,8)_example.csv']:\n",
    "    df1 = pd.read_csv(join(dr, f), sep=None, engine='python')\n",
    "    df1.to_csv(join(cor_ann, f), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_raw = './uni_raw'\n",
    "\n",
    "def make_unimorph(new_raw, cor_ann, raw_dr):\n",
    "    for f in listdir(raw_dr):\n",
    "        df = pd.read_csv(join(raw_dr, f), sep=None, engine='python')\n",
    "        f, ck = check_pos(df, f, new_new_raw)\n",
    "\n",
    "    return fn, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ_000001_(55,16)_example.csv\n",
      "(33, 6)\n",
      "(32, 6)\n",
      "ADJ_000003_(69,14)_example.csv\n",
      "(33, 6)\n",
      "(32, 6)\n",
      "ADJ_000001_(21,8)_example.csv\n",
      "ADJ_000001_(46,16)_example.csv\n",
      "(33, 6)\n",
      "(32, 6)\n",
      "ADJ_000001_(30,8)_example.csv\n",
      "(15, 6)\n",
      "(29, 8)\n",
      "ADJ_000001_(68,14)_example.csv\n",
      "(33, 6)\n",
      "(32, 6)\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import re\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# gather arguments\n",
    "'''parser = argparse.ArgumentParser(\n",
    "    description=\"Extract tabular paradigms from annotated templates.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-candidates_dir\",\n",
    "    action=\"store\",\n",
    "    dest=\"candidates_dir\",\n",
    "    help=\"Location of candidate html pages.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-annotation_dir\",\n",
    "    action=\"store\",\n",
    "    dest=\"annotation_dir\",\n",
    "    help=\"Location of raw/annotated table templates.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-language\", action=\"store\", dest=\"language\", help=\"Language to grab.\"\n",
    ")\n",
    "args = parser.parse_args()'''\n",
    "\n",
    "# regular expressions\n",
    "lempat = r\"<h1.*?>(.*?)</h1>\"\n",
    "locpat1 = r\"</h2>.*?</h2>\"\n",
    "locpat2 = r\"</h2>.*?</body>\"\n",
    "pospat = r\">(.*?)</h3>\"\n",
    "\n",
    "\n",
    "# input tables\n",
    "orig_dir = \"./nnew_auto_raw_tables/\"\n",
    "  # original example tables for comparison #CHANGE\n",
    "done_dir = \"./nnew_auto_annotated_tables/\"\n",
    " # annotated example tables #CHANGE\n",
    "\n",
    "# output directory\n",
    "out_dir = \"./tabular_results/\"  # output data goes here\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "candidates_dir = \"./candidate_pages/\"\n",
    "\n",
    "# language\n",
    "language = \"Korean\"\n",
    "\n",
    "# output file\n",
    "fout_name = out_dir + language + \"_tabular_paradigms.txt\"\n",
    "fout = codecs.open(fout_name, \"wb\", \"utf-8\")\n",
    "\n",
    "# get the table patterns\n",
    "tables = {}\n",
    "\n",
    "\n",
    "def clean_table(df):\n",
    "    \n",
    "    if \"Indicative\" not in df[df.columns[0]].values:\n",
    "        return df, \"\"\n",
    "    if df[df.columns[0]].value_counts()['Indicative'] > 4:\n",
    "        return df, \"\"\n",
    "    if \"Imperative\" in df[df.columns[0]].values or \"Hortative\" in df[df.columns[0]].values or \"Motive\" in df[df.columns[0]].values:\n",
    "        pos = \"V\"\n",
    "    else:\n",
    "        pos = \"ADJ\"\n",
    "            \n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        if \"Formal non-polite(해라체)\" in list(row[1].unique()) or \"Formal non-polite|(해라체)\" in list(row[1].unique()):\n",
    "            if pos == \"ADJ\":\n",
    "                st = i\n",
    "                en = i + 34\n",
    "            if pos == \"V\":\n",
    "                st = i\n",
    "                en = i + 40\n",
    "            break\n",
    "                \n",
    "    df = pd.DataFrame([i[1].values[0:6] for i in list(df.iterrows())[st:en]])\n",
    "        \n",
    "    for i in df.columns:\n",
    "        df[i] = df[i].str.replace(u\"(([\\u3131-\\u3163\\uac00-\\ud7a3]+, )?[\\u3131-\\u3163\\uac00-\\ud7a3]+)(-?[a-z, ]+)\", u\"\\g<1>\", regex=True)\n",
    "        \n",
    "    #df.to_csv(join(new_raw, f[:-3] + 'tsv'), sep='\\t', header=False, index=False)\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = new_header\n",
    "    return df, pos\n",
    "\n",
    "\n",
    "# loop through annotated directory\n",
    "for n in os.listdir(done_dir):\n",
    "    if n.endswith(\"csv\"):\n",
    "        mod_set = {}\n",
    "        try:\n",
    "            odata = pd.read_csv(orig_dir + n, sep=None, engine='python', header=0).fillna(\n",
    "                \"\"\n",
    "            )\n",
    "            ddata = pd.read_csv(done_dir + n, sep=None, engine='python', header=0).fillna(\n",
    "                \"\"\n",
    "            )\n",
    "        except:\n",
    "            print(n)\n",
    "            #raise\n",
    "            continue\n",
    "        try:\n",
    "            assert odata.shape == ddata.shape  # make sure we really got the same table\n",
    "        except:\n",
    "            print(n)\n",
    "            #print(odata)\n",
    "            #print(ddata)\n",
    "            print(odata.shape)\n",
    "            print(ddata.shape)\n",
    "            #raise\n",
    "            continue\n",
    "        for i in range(odata.shape[0]):\n",
    "            for j in range(odata.shape[1]):\n",
    "                if odata.iloc[i, j] != ddata.iloc[i, j]:\n",
    "                    mod_set[(i, j)] = ddata.iloc[i, j]\n",
    "\n",
    "        # if there were some actual annotations store them\n",
    "        if len(mod_set) > 0:\n",
    "            if n.startswith(\"N_\"):\n",
    "                tables[\"N_\" + str(odata.shape)] = mod_set\n",
    "            if n.startswith(\"ADJ_\"):\n",
    "                tables[\"ADJ_\" + str(odata.shape)] = mod_set\n",
    "            if n.startswith(\"V_\"):\n",
    "                tables[\"V_\" + str(odata.shape)] = mod_set\n",
    "\n",
    "k = 0\n",
    "# #loop through languages\n",
    "lnames = os.listdir(candidates_dir)  # CHANGE\n",
    "for ln in lnames:\n",
    "    if ln == language:\n",
    "        names = os.listdir(os.path.join(candidates_dir, ln))  # CHANGE\n",
    "        # loop through language pages\n",
    "        # count = 0\n",
    "        for n in names:\n",
    "            # if n.startswith('candidate_33623.html'):\n",
    "            if n.startswith(\"candidate\"):\n",
    "                fin = codecs.open(\n",
    "                    os.path.join(candidates_dir, ln, n), \"rb\", \"utf-8\"\n",
    "                )  # CHANGE\n",
    "                page = fin.read()#.replace(\"<br>\", \"|\")\n",
    "                fin.close()\n",
    "\n",
    "                # get the lemma from the page\n",
    "                match = re.search(lempat, page, flags=re.U | re.DOTALL)\n",
    "                if match:\n",
    "                    lemma = match.group(1)\n",
    "                    # print lemma\n",
    "                    match = re.search(ln + locpat1, page, flags=re.U | re.DOTALL)\n",
    "                    if not match:\n",
    "                        match = re.search(ln + locpat2, page, flags=re.U | re.DOTALL)\n",
    "                    if match:\n",
    "                        text = match.group()\n",
    "                        text = re.sub('colspan=\"100%\"', 'colspan=\"0\"', text, flags=re.U | re.DOTALL)\n",
    "                        try:\n",
    "                            data = pd.read_html(text)\n",
    "                            if len(data) >= 1:\n",
    "                                data = pd.concat(data)\n",
    "                                data, pos = clean_table(data)\n",
    "                                shape = data.shape\n",
    "                                if pos + '_' + str(shape) in tables:\n",
    "                                    for mod, feats in tables[pos + '_' + str(shape)].items():\n",
    "                                        words = data.iloc[mod[0], mod[1]]\n",
    "                                        if not pd.isnull(words):\n",
    "                                            for word in words.split(', '): \n",
    "                                                if re.match(u'[a-zA-Z]+', word) is None and word != '':\n",
    "                                                    fout.write(\n",
    "                                                        re.search('>([^<]+)<', lemma).group(1)\n",
    "                                                        + \"\\t\"\n",
    "                                                        + word\n",
    "                                                        + \"\\t\"\n",
    "                                                        + feats.split(', ')[0]\n",
    "                                                        + \"\\n\"\n",
    "                                                )\n",
    "                            else:\n",
    "                                k+=1\n",
    "                        except:\n",
    "                                # if data.shape == (6,8):\n",
    "                                # \tprint data\n",
    "                            #raise\n",
    "                            k+=1\n",
    "                            #fout.write(\"----\\t----\\t----\\n\")\n",
    "                            #fout.write(\"\\n\")\n",
    "                            pass\n",
    "\n",
    "\n",
    "# clean up\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_raw = './non_lemmas_raw_tables/'\n",
    "nl_anno = './non_lemmas_annotated_tables/'\n",
    "fout_name = out_dir + language + \"_non_lemmas_tabular_paradigms.txt\"\n",
    "fout = codecs.open(fout_name, \"wb\", \"utf-8\")\n",
    "for n in os.listdir(nl_raw):\n",
    "    with open (join(nl_raw, n), encoding='utf-8') as f:\n",
    "        word = f.read().split('\\n')[0]\n",
    "    with open (join(nl_anno, n), encoding='utf-8') as f:\n",
    "        feats = f.read().split('\\n')[0]\n",
    "    fout.write('lemma' + \"\\t\" + word + \"\\t\" + feats + \"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240151"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "240139 + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
